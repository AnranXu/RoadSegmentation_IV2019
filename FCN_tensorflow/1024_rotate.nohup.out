Step: 150, Train_loss:4.26633
Step: 200, Train_loss:2.23029
Step: 250, Train_loss:1.4764
Step: 300, Train_loss:1.42189
Step: 350, Train_loss:1.25515
Step: 400, Train_loss:0.87966
Step: 450, Train_loss:0.991762
Step: 500, Train_loss:0.896287
Step: 550, Train_loss:0.837688
Step: 600, Train_loss:0.865986
Step: 650, Train_loss:0.842073
Step: 700, Train_loss:0.587416
Step: 750, Train_loss:0.673263
Step: 800, Train_loss:0.62573
Step: 850, Train_loss:0.618535
Step: 900, Train_loss:0.601458
Step: 950, Train_loss:0.737693
Step: 1000, Train_loss:0.614607
Step: 1050, Train_loss:0.612394
Step: 1100, Train_loss:0.60837
Step: 1150, Train_loss:0.517898
Step: 1200, Train_loss:0.55871
Step: 1250, Train_loss:0.484267
Step: 1300, Train_loss:0.578054
Step: 1350, Train_loss:0.509559
Step: 1400, Train_loss:0.636629
Step: 1450, Train_loss:0.447548
Step: 1500, Train_loss:0.551821
Step: 1550, Train_loss:0.438394
Step: 1600, Train_loss:0.400753
Step: 1650, Train_loss:0.505927
Step: 1700, Train_loss:0.500696
Step: 1750, Train_loss:0.48114
Step: 1800, Train_loss:0.617903
Step: 1850, Train_loss:0.507173
Step: 1900, Train_loss:0.431936
Step: 1950, Train_loss:0.576855
Step: 2000, Train_loss:0.35193
Step: 2050, Train_loss:0.486028
Step: 2100, Train_loss:0.481719
Step: 2150, Train_loss:0.457623
Step: 2200, Train_loss:0.464563
Step: 2250, Train_loss:0.462444
Step: 2300, Train_loss:0.376372
Step: 2350, Train_loss:0.45568
Step: 2400, Train_loss:0.456782
Step: 2450, Train_loss:0.521433
Step: 2500, Train_loss:0.520348
Step: 2550, Train_loss:0.482126
Step: 2600, Train_loss:0.543955
Step: 2650, Train_loss:0.463556
Step: 2700, Train_loss:0.487876
Step: 2750, Train_loss:0.407639
Step: 2800, Train_loss:0.503133
Step: 2850, Train_loss:0.493874
Step: 2900, Train_loss:0.460215
Step: 2950, Train_loss:0.333052
Step: 3000, Train_loss:0.447722
Step: 3050, Train_loss:0.420807
Step: 3100, Train_loss:0.419358
Step: 3150, Train_loss:0.430128
Step: 3200, Train_loss:0.477512
Step: 3250, Train_loss:0.442015
Step: 3300, Train_loss:0.398251
Step: 3350, Train_loss:0.418855
Step: 3400, Train_loss:0.437451
Step: 3450, Train_loss:0.475316
Step: 3500, Train_loss:0.429343
Step: 3550, Train_loss:0.452506
Step: 3600, Train_loss:0.428762
Step: 3650, Train_loss:0.414367
Step: 3700, Train_loss:0.470707
Step: 3750, Train_loss:0.55502
Step: 3800, Train_loss:0.415447
Step: 3850, Train_loss:0.410756
Step: 3900, Train_loss:0.361187
Step: 3950, Train_loss:0.371808
Step: 4000, Train_loss:0.370756
Step: 4050, Train_loss:0.357938
Step: 4100, Train_loss:0.427001
Step: 4150, Train_loss:0.442031
Step: 4200, Train_loss:0.446806
Step: 4250, Train_loss:0.466707
Step: 4300, Train_loss:0.351908
Step: 4350, Train_loss:0.412502
Step: 4400, Train_loss:0.394175
Step: 4450, Train_loss:0.430376
Step: 4500, Train_loss:0.379939
Step: 4550, Train_loss:0.410389
Step: 4600, Train_loss:0.359041
Step: 4650, Train_loss:0.399829
Step: 4700, Train_loss:0.523337
Step: 4750, Train_loss:0.455323
Step: 4800, Train_loss:0.42601
Step: 4850, Train_loss:0.413447
Step: 4900, Train_loss:0.386745
Step: 4950, Train_loss:0.413044
Step: 5000, Train_loss:0.441933
2018-10-24 21:18:41.738259 ---> Validation_loss: 0.383976
* test result update.
Step: 5050, Train_loss:0.369062
Step: 5100, Train_loss:0.412246
Step: 5150, Train_loss:0.298575
Step: 5200, Train_loss:0.420053
Step: 5250, Train_loss:0.396586
Step: 5300, Train_loss:0.376485
Step: 5350, Train_loss:0.29049
Step: 5400, Train_loss:0.387806
Step: 5450, Train_loss:0.339005
Step: 5500, Train_loss:0.453633
Step: 5550, Train_loss:0.371875
Step: 5600, Train_loss:0.396652
Step: 5650, Train_loss:0.452181
Step: 5700, Train_loss:0.361284
Step: 5750, Train_loss:0.389167
Step: 5800, Train_loss:0.363424
Step: 5850, Train_loss:0.395029
Step: 5900, Train_loss:0.33751
Step: 5950, Train_loss:0.442966
Step: 6000, Train_loss:0.362125
Step: 6050, Train_loss:0.31625
Step: 6100, Train_loss:0.369435
Step: 6150, Train_loss:0.40365
Step: 6200, Train_loss:0.345696
Step: 6250, Train_loss:0.402898
Step: 6300, Train_loss:0.294647
Step: 6350, Train_loss:0.359252
Step: 6400, Train_loss:0.411921
Step: 6450, Train_loss:0.306885
Step: 6500, Train_loss:0.372058
Step: 6550, Train_loss:0.421246
Step: 6600, Train_loss:0.313605
Step: 6650, Train_loss:0.373701
Step: 6700, Train_loss:0.438645
Step: 6750, Train_loss:0.453144
Step: 6800, Train_loss:0.283288
Step: 6850, Train_loss:0.35022
Step: 6900, Train_loss:0.450111
Step: 6950, Train_loss:0.371504
Step: 7000, Train_loss:0.367827
Step: 7050, Train_loss:0.414357
Step: 7100, Train_loss:0.340781
Step: 7150, Train_loss:0.332928
Step: 7200, Train_loss:0.447503
Step: 7250, Train_loss:0.378151
Step: 7300, Train_loss:0.403983
Step: 7350, Train_loss:0.35329
Step: 7400, Train_loss:0.320776
Step: 7450, Train_loss:0.404541
Step: 7500, Train_loss:0.441161
Step: 7550, Train_loss:0.405671
Step: 7600, Train_loss:0.328552
Step: 7650, Train_loss:0.426863
Step: 7700, Train_loss:0.380764
Step: 7750, Train_loss:0.348257
Step: 7800, Train_loss:0.327915
****************** Epochs completed: 10******************
Step: 7850, Train_loss:0.390733
Step: 7900, Train_loss:0.34435
Step: 7950, Train_loss:0.417619
Step: 8000, Train_loss:0.349749
Step: 8050, Train_loss:0.355586
Step: 8100, Train_loss:0.40545
Step: 8150, Train_loss:0.26972
Step: 8200, Train_loss:0.332184
Step: 8250, Train_loss:0.275047
Step: 8300, Train_loss:0.288519
Step: 8350, Train_loss:0.299103
Step: 8400, Train_loss:0.28394
Step: 8450, Train_loss:0.321255
Step: 8500, Train_loss:0.316728
Step: 8550, Train_loss:0.351195
Step: 8600, Train_loss:0.372712
Step: 8650, Train_loss:0.322398
Step: 8700, Train_loss:0.365306
Step: 8750, Train_loss:0.362746
Step: 8800, Train_loss:0.240742
Step: 8850, Train_loss:0.349378
Step: 8900, Train_loss:0.346374
Step: 8950, Train_loss:0.32451
Step: 9000, Train_loss:0.403359
Step: 9050, Train_loss:0.32161
Step: 9100, Train_loss:0.298709
Step: 9150, Train_loss:0.296858
Step: 9200, Train_loss:0.388921
Step: 9250, Train_loss:0.304721
Step: 9300, Train_loss:0.30077
Step: 9350, Train_loss:0.292102
Step: 9400, Train_loss:0.372384
Step: 9450, Train_loss:0.259989
Step: 9500, Train_loss:0.337247
Step: 9550, Train_loss:0.272549
Step: 9600, Train_loss:0.370268
Step: 9650, Train_loss:0.363352
Step: 9700, Train_loss:0.424841
Step: 9750, Train_loss:0.335444
Step: 9800, Train_loss:0.400693
Step: 9850, Train_loss:0.34721
Step: 9900, Train_loss:0.263243
Step: 9950, Train_loss:0.338328
Step: 10000, Train_loss:0.349952
2018-10-24 21:46:03.813345 ---> Validation_loss: 0.390357
* test result update.
Step: 10050, Train_loss:0.349919
Step: 10100, Train_loss:0.320438
Step: 10150, Train_loss:0.255265
Step: 10200, Train_loss:0.30224
Step: 10250, Train_loss:0.316513
Step: 10300, Train_loss:0.332731
Step: 10350, Train_loss:0.357829
Step: 10400, Train_loss:0.292741
Step: 10450, Train_loss:0.352174
Step: 10500, Train_loss:0.29772
Step: 10550, Train_loss:0.288615
Step: 10600, Train_loss:0.406083
Step: 10650, Train_loss:0.404807
Step: 10700, Train_loss:0.293078
Step: 10750, Train_loss:0.297906
Step: 10800, Train_loss:0.323782
Step: 10850, Train_loss:0.293299
Step: 10900, Train_loss:0.329413
Step: 10950, Train_loss:0.268943
Step: 11000, Train_loss:0.298524
Step: 11050, Train_loss:0.361746
Step: 11100, Train_loss:0.298109
Step: 11150, Train_loss:0.281226
Step: 11200, Train_loss:0.241674
Step: 11250, Train_loss:0.291115
Step: 11300, Train_loss:0.242128
Step: 11350, Train_loss:0.37064
Step: 11400, Train_loss:0.347957
Step: 11450, Train_loss:0.312641
Step: 11500, Train_loss:0.376403
Step: 11550, Train_loss:0.33834
Step: 11600, Train_loss:0.253029
Step: 11650, Train_loss:0.303407
Step: 11700, Train_loss:0.367965
Step: 11750, Train_loss:0.248542
Step: 11800, Train_loss:0.318356
Step: 11850, Train_loss:0.297407
Step: 11900, Train_loss:0.295324
Step: 11950, Train_loss:0.38629
Step: 12000, Train_loss:0.29684
Step: 12050, Train_loss:0.287763
Step: 12100, Train_loss:0.329762
Step: 12150, Train_loss:0.339827
Step: 12200, Train_loss:0.240191
Step: 12250, Train_loss:0.334619
Step: 12300, Train_loss:0.328968
Step: 12350, Train_loss:0.31073
Step: 12400, Train_loss:0.39764
Step: 12450, Train_loss:0.261986
Step: 12500, Train_loss:0.284905
Step: 12550, Train_loss:0.310457
Step: 12600, Train_loss:0.274166
Step: 12650, Train_loss:0.283736
Step: 12700, Train_loss:0.30117
Step: 12750, Train_loss:0.315975
Step: 12800, Train_loss:0.314785
Step: 12850, Train_loss:0.304975
Step: 12900, Train_loss:0.355889
Step: 12950, Train_loss:0.268149
Step: 13000, Train_loss:0.338382
Step: 13050, Train_loss:0.331735
Step: 13100, Train_loss:0.311353
Step: 13150, Train_loss:0.329259
Step: 13200, Train_loss:0.31315
Step: 13250, Train_loss:0.270937
Step: 13300, Train_loss:0.277533
Step: 13350, Train_loss:0.311029
Step: 13400, Train_loss:0.266895
Step: 13450, Train_loss:0.261417
Step: 13500, Train_loss:0.306983
Step: 13550, Train_loss:0.297134
Step: 13600, Train_loss:0.325854
Step: 13650, Train_loss:0.253308
Step: 13700, Train_loss:0.350205
Step: 13750, Train_loss:0.327648
Step: 13800, Train_loss:0.279528
Step: 13850, Train_loss:0.348962
Step: 13900, Train_loss:0.248608
Step: 13950, Train_loss:0.344325
Step: 14000, Train_loss:0.360379
Step: 14050, Train_loss:0.258393
Step: 14100, Train_loss:0.204793
Step: 14150, Train_loss:0.236495
Step: 14200, Train_loss:0.332384
Step: 14250, Train_loss:0.22184
Step: 14300, Train_loss:0.283075
Step: 14350, Train_loss:0.303098
Step: 14400, Train_loss:0.273987
Step: 14450, Train_loss:0.287436
Step: 14500, Train_loss:0.298705
Step: 14550, Train_loss:0.291766
Step: 14600, Train_loss:0.323807
Step: 14650, Train_loss:0.249458
Step: 14700, Train_loss:0.314341
Step: 14750, Train_loss:0.292819
Step: 14800, Train_loss:0.372074
Step: 14850, Train_loss:0.362098
Step: 14900, Train_loss:0.306364
Step: 14950, Train_loss:0.327403
Step: 15000, Train_loss:0.296107
2018-10-24 22:13:31.019475 ---> Validation_loss: 0.439413
* test result update.
Step: 15050, Train_loss:0.347281
Step: 15100, Train_loss:0.267988
Step: 15150, Train_loss:0.296167
Step: 15200, Train_loss:0.23274
Step: 15250, Train_loss:0.253228
Step: 15300, Train_loss:0.261279
Step: 15350, Train_loss:0.295472
Step: 15400, Train_loss:0.312555
Step: 15450, Train_loss:0.334202
Step: 15500, Train_loss:0.263636
Step: 15550, Train_loss:0.321048
Step: 15600, Train_loss:0.242643
Step: 15650, Train_loss:0.342994
****************** Epochs completed: 20******************
Step: 15700, Train_loss:0.275482
Step: 15750, Train_loss:0.290627
Step: 15800, Train_loss:0.326895
Step: 15850, Train_loss:0.30557
Step: 15900, Train_loss:0.224928
Step: 15950, Train_loss:0.311689
Step: 16000, Train_loss:0.346932
Step: 16050, Train_loss:0.330161
Step: 16100, Train_loss:0.344141
Step: 16150, Train_loss:0.312723
Step: 16200, Train_loss:0.274217
Step: 16250, Train_loss:0.225773
Step: 16300, Train_loss:0.296814
Step: 16350, Train_loss:0.304944
Step: 16400, Train_loss:0.32352
Step: 16450, Train_loss:0.2719
Step: 16500, Train_loss:0.215742
Step: 16550, Train_loss:0.303992
Step: 16600, Train_loss:0.246378
Step: 16650, Train_loss:0.262376
Step: 16700, Train_loss:0.265158
Step: 16750, Train_loss:0.257784
Step: 16800, Train_loss:0.278883
Step: 16850, Train_loss:0.303702
Step: 16900, Train_loss:0.259424
Step: 16950, Train_loss:0.230705
Step: 17000, Train_loss:0.328174
Step: 17050, Train_loss:0.294255
Step: 17100, Train_loss:0.285669
Step: 17150, Train_loss:0.286737
Step: 17200, Train_loss:0.261492
Step: 17250, Train_loss:0.214636
Step: 17300, Train_loss:0.320246
Step: 17350, Train_loss:0.251292
Step: 17400, Train_loss:0.289725
Step: 17450, Train_loss:0.29828
Step: 17500, Train_loss:0.263245
Step: 17550, Train_loss:0.27468
Step: 17600, Train_loss:0.337205
Step: 17650, Train_loss:0.319745
Step: 17700, Train_loss:0.270507
Step: 17750, Train_loss:0.282184
Step: 17800, Train_loss:0.258321
Step: 17850, Train_loss:0.276095
Step: 17900, Train_loss:0.308841
Step: 17950, Train_loss:0.204197
Step: 18000, Train_loss:0.252796
Step: 18050, Train_loss:0.240654
Step: 18100, Train_loss:0.232045
Step: 18150, Train_loss:0.232581
Step: 18200, Train_loss:0.285037
Step: 18250, Train_loss:0.281621
Step: 18300, Train_loss:0.309946
Step: 18350, Train_loss:0.303316
Step: 18400, Train_loss:0.248128
Step: 18450, Train_loss:0.241808
Step: 18500, Train_loss:0.318644
Step: 18550, Train_loss:0.213881
Step: 18600, Train_loss:0.276926
Step: 18650, Train_loss:0.254785
Step: 18700, Train_loss:0.206537
Step: 18750, Train_loss:0.260542
Step: 18800, Train_loss:0.349453
Step: 18850, Train_loss:0.332274
Step: 18900, Train_loss:0.332708
Step: 18950, Train_loss:0.303467
Step: 19000, Train_loss:0.222676
Step: 19050, Train_loss:0.252103
Step: 19100, Train_loss:0.274254
Step: 19150, Train_loss:0.223143
Step: 19200, Train_loss:0.28205
Step: 19250, Train_loss:0.252487
Step: 19300, Train_loss:0.270658
Step: 19350, Train_loss:0.298852
Step: 19400, Train_loss:0.243726
Step: 19450, Train_loss:0.249397
Step: 19500, Train_loss:0.297681
Step: 19550, Train_loss:0.284541
Step: 19600, Train_loss:0.250183
Step: 19650, Train_loss:0.254146
Step: 19700, Train_loss:0.231539
Step: 19750, Train_loss:0.245274
Step: 19800, Train_loss:0.271042
Step: 19850, Train_loss:0.30362
Step: 19900, Train_loss:0.257004
Step: 19950, Train_loss:0.291552
Step: 20000, Train_loss:0.321224
2018-10-24 22:40:56.788462 ---> Validation_loss: 0.641819
* test result update.
Step: 20050, Train_loss:0.305575
Step: 20100, Train_loss:0.293908
Step: 20150, Train_loss:0.216534
Step: 20200, Train_loss:0.291467
Step: 20250, Train_loss:0.271736
Step: 20300, Train_loss:0.297981
Step: 20350, Train_loss:0.317296
Step: 20400, Train_loss:0.197516
Step: 20450, Train_loss:0.225814
Step: 20500, Train_loss:0.25572
Step: 20550, Train_loss:0.289466
Step: 20600, Train_loss:0.326193
Step: 20650, Train_loss:0.303633
Step: 20700, Train_loss:0.237073
Step: 20750, Train_loss:0.257911
Step: 20800, Train_loss:0.265566
Step: 20850, Train_loss:0.277839
Step: 20900, Train_loss:0.290489
Step: 20950, Train_loss:0.280293
Step: 21000, Train_loss:0.282885
Step: 21050, Train_loss:0.308657
Step: 21100, Train_loss:0.224599
Step: 21150, Train_loss:0.290015
Step: 21200, Train_loss:0.267382
Step: 21250, Train_loss:0.370659
Step: 21300, Train_loss:0.274926
Step: 21350, Train_loss:0.307485
Step: 21400, Train_loss:0.269282
Step: 21450, Train_loss:0.282421
Step: 21500, Train_loss:0.297665
Step: 21550, Train_loss:0.289437
Step: 21600, Train_loss:0.269506
Step: 21650, Train_loss:0.236558
Step: 21700, Train_loss:0.300781
Step: 21750, Train_loss:0.288267
Step: 21800, Train_loss:0.225721
Step: 21850, Train_loss:0.260161
Step: 21900, Train_loss:0.281061
Step: 21950, Train_loss:0.243082
Step: 22000, Train_loss:0.246015
Step: 22050, Train_loss:0.250278
Step: 22100, Train_loss:0.254346
Step: 22150, Train_loss:0.216909
Step: 22200, Train_loss:0.333123
Step: 22250, Train_loss:0.221929
Step: 22300, Train_loss:0.264699
Step: 22350, Train_loss:0.229444
Step: 22400, Train_loss:0.231319
Step: 22450, Train_loss:0.24945
Step: 22500, Train_loss:0.254467
Step: 22550, Train_loss:0.297987
Step: 22600, Train_loss:0.236195
Step: 22650, Train_loss:0.241939
Step: 22700, Train_loss:0.298955
Step: 22750, Train_loss:0.263185
Step: 22800, Train_loss:0.225952
Step: 22850, Train_loss:0.187791
Step: 22900, Train_loss:0.211204
Step: 22950, Train_loss:0.270958
Step: 23000, Train_loss:0.307485
Step: 23050, Train_loss:0.229944
Step: 23100, Train_loss:0.228179
Step: 23150, Train_loss:0.285199
Step: 23200, Train_loss:0.235703
Step: 23250, Train_loss:0.267424
Step: 23300, Train_loss:0.288683
Step: 23350, Train_loss:0.229811
Step: 23400, Train_loss:0.304868
Step: 23450, Train_loss:0.276342
Step: 23500, Train_loss:0.197599
****************** Epochs completed: 30******************
Step: 23550, Train_loss:0.190108
Step: 23600, Train_loss:0.186982
Step: 23650, Train_loss:0.213693
Step: 23700, Train_loss:0.230754
Step: 23750, Train_loss:0.221837
Step: 23800, Train_loss:0.233578
Step: 23850, Train_loss:0.231469
Step: 23900, Train_loss:0.204012
Step: 23950, Train_loss:0.260476
Step: 24000, Train_loss:0.287795
Step: 24050, Train_loss:0.247663
Step: 24100, Train_loss:0.260364
Step: 24150, Train_loss:0.280327
Step: 24200, Train_loss:0.201339
Step: 24250, Train_loss:0.273101
Step: 24300, Train_loss:0.248246
Step: 24350, Train_loss:0.308217
Step: 24400, Train_loss:0.261954
Step: 24450, Train_loss:0.232875
Step: 24500, Train_loss:0.232417
Step: 24550, Train_loss:0.21232
Step: 24600, Train_loss:0.226697
Step: 24650, Train_loss:0.247592
Step: 24700, Train_loss:0.218459
Step: 24750, Train_loss:0.213163
Step: 24800, Train_loss:0.243666
Step: 24850, Train_loss:0.19921
Step: 24900, Train_loss:0.242324
Step: 24950, Train_loss:0.287342
Step: 25000, Train_loss:0.294923
2018-10-24 23:08:24.015566 ---> Validation_loss: 0.786876
* test result update.
Step: 25050, Train_loss:0.212482
Step: 25100, Train_loss:0.294381
Step: 25150, Train_loss:0.278657
Step: 25200, Train_loss:0.248103
Step: 25250, Train_loss:0.231094
Step: 25300, Train_loss:0.215797
Step: 25350, Train_loss:0.255788
Step: 25400, Train_loss:0.340923
Step: 25450, Train_loss:0.215945
Step: 25500, Train_loss:0.298384
Step: 25550, Train_loss:0.212624
Step: 25600, Train_loss:0.261238
Step: 25650, Train_loss:0.307374
Step: 25700, Train_loss:0.237019
Step: 25750, Train_loss:0.243229
Step: 25800, Train_loss:0.202719
Step: 25850, Train_loss:0.228472
Step: 25900, Train_loss:0.258648
Step: 25950, Train_loss:0.258531
Step: 26000, Train_loss:0.285664
Step: 26050, Train_loss:0.277785
Step: 26100, Train_loss:0.286763
Step: 26150, Train_loss:0.278516
Step: 26200, Train_loss:0.234431
Step: 26250, Train_loss:0.206666
Step: 26300, Train_loss:0.322953
Step: 26350, Train_loss:0.260337
Step: 26400, Train_loss:0.305361
Step: 26450, Train_loss:0.251339
Step: 26500, Train_loss:0.218746
Step: 26550, Train_loss:0.276475
Step: 26600, Train_loss:0.239004
Step: 26650, Train_loss:0.197071
Step: 26700, Train_loss:0.222589
Step: 26750, Train_loss:0.244946
Step: 26800, Train_loss:0.205016
Step: 26850, Train_loss:0.214271
Step: 26900, Train_loss:0.222342
Step: 26950, Train_loss:0.232016
Step: 27000, Train_loss:0.329506
Step: 27050, Train_loss:0.234759
Step: 27100, Train_loss:0.219705
Step: 27150, Train_loss:0.228134
Step: 27200, Train_loss:0.327279
Step: 27250, Train_loss:0.283732
Step: 27300, Train_loss:0.217936
Step: 27350, Train_loss:0.294632
Step: 27400, Train_loss:0.210402
Step: 27450, Train_loss:0.259213
Step: 27500, Train_loss:0.204765
Step: 27550, Train_loss:0.235302
Step: 27600, Train_loss:0.19982
Step: 27650, Train_loss:0.234068
Step: 27700, Train_loss:0.209535
Step: 27750, Train_loss:0.21217
Step: 27800, Train_loss:0.272353
Step: 27850, Train_loss:0.250386
Step: 27900, Train_loss:0.254793
Step: 27950, Train_loss:0.224357
Step: 28000, Train_loss:0.263319
Step: 28050, Train_loss:0.230343
Step: 28100, Train_loss:0.285109
Step: 28150, Train_loss:0.291528
Step: 28200, Train_loss:0.209526
Step: 28250, Train_loss:0.289474
Step: 28300, Train_loss:0.222539
Step: 28350, Train_loss:0.240112
Step: 28400, Train_loss:0.298635
Step: 28450, Train_loss:0.211179
Step: 28500, Train_loss:0.222818
Step: 28550, Train_loss:0.240885
Step: 28600, Train_loss:0.258491
Step: 28650, Train_loss:0.230666
Step: 28700, Train_loss:0.24954
Step: 28750, Train_loss:0.262688
Step: 28800, Train_loss:0.228726
Step: 28850, Train_loss:0.283255
Step: 28900, Train_loss:0.192046
Step: 28950, Train_loss:0.245207
Step: 29000, Train_loss:0.279944
Step: 29050, Train_loss:0.291403
Step: 29100, Train_loss:0.257204
Step: 29150, Train_loss:0.250296
Step: 29200, Train_loss:0.190652
Step: 29250, Train_loss:0.233235
Step: 29300, Train_loss:0.203235
Step: 29350, Train_loss:0.213077
Step: 29400, Train_loss:0.304138
Step: 29450, Train_loss:0.255898
Step: 29500, Train_loss:0.227083
Step: 29550, Train_loss:0.22859
Step: 29600, Train_loss:0.31614
Step: 29650, Train_loss:0.219718
Step: 29700, Train_loss:0.267812
Step: 29750, Train_loss:0.267574
Step: 29800, Train_loss:0.281009
Step: 29850, Train_loss:0.248539
Step: 29900, Train_loss:0.265302
Step: 29950, Train_loss:0.211092
Step: 30000, Train_loss:0.195291
2018-10-24 23:35:50.014949 ---> Validation_loss: 0.847646
* test result update.
Step: 30050, Train_loss:0.274391
Step: 30100, Train_loss:0.265616
Step: 30150, Train_loss:0.190803
Step: 30200, Train_loss:0.232133
Step: 30250, Train_loss:0.23598
Step: 30300, Train_loss:0.210323
Step: 30350, Train_loss:0.244238
Step: 30400, Train_loss:0.278066
Step: 30450, Train_loss:0.318701
Step: 30500, Train_loss:0.243026
Step: 30550, Train_loss:0.234295
Step: 30600, Train_loss:0.235768
Step: 30650, Train_loss:0.227905
Step: 30700, Train_loss:0.228962
Step: 30750, Train_loss:0.225273
Step: 30800, Train_loss:0.213885
Step: 30850, Train_loss:0.255978
Step: 30900, Train_loss:0.1888
Step: 30950, Train_loss:0.241051
Step: 31000, Train_loss:0.239585
Step: 31050, Train_loss:0.186366
Step: 31100, Train_loss:0.249194
Step: 31150, Train_loss:0.216131
Step: 31200, Train_loss:0.199388
Step: 31250, Train_loss:0.222036
Step: 31300, Train_loss:0.278864
Step: 31350, Train_loss:0.252318
****************** Epochs completed: 40******************
Step: 31400, Train_loss:0.286094
Step: 31450, Train_loss:0.211057
Step: 31500, Train_loss:0.203929
Step: 31550, Train_loss:0.254847
Step: 31600, Train_loss:0.191305
Step: 31650, Train_loss:0.190111
Step: 31700, Train_loss:0.272466
Step: 31750, Train_loss:0.225217
Step: 31800, Train_loss:0.214276
Step: 31850, Train_loss:0.270601
Step: 31900, Train_loss:0.230045
Step: 31950, Train_loss:0.255872
Step: 32000, Train_loss:0.202916
Step: 32050, Train_loss:0.195766
Step: 32100, Train_loss:0.228107
Step: 32150, Train_loss:0.192995
Step: 32200, Train_loss:0.190584
Step: 32250, Train_loss:0.223513
Step: 32300, Train_loss:0.254351
Step: 32350, Train_loss:0.193401
Step: 32400, Train_loss:0.256964
Step: 32450, Train_loss:0.191675
Step: 32500, Train_loss:0.237184
Step: 32550, Train_loss:0.237636
Step: 32600, Train_loss:0.239588
Step: 32650, Train_loss:0.230859
Step: 32700, Train_loss:0.227601
Step: 32750, Train_loss:0.264757
Step: 32800, Train_loss:0.211033
Step: 32850, Train_loss:0.252798
Step: 32900, Train_loss:0.275271
Step: 32950, Train_loss:0.207011
Step: 33000, Train_loss:0.2374
Step: 33050, Train_loss:0.207512
Step: 33100, Train_loss:0.30356
Step: 33150, Train_loss:0.190992
Step: 33200, Train_loss:0.238854
Step: 33250, Train_loss:0.254347
Step: 33300, Train_loss:0.245202
Step: 33350, Train_loss:0.228952
Step: 33400, Train_loss:0.229611
Step: 33450, Train_loss:0.17774
Step: 33500, Train_loss:0.215593
Step: 33550, Train_loss:0.208838
Step: 33600, Train_loss:0.224618
Step: 33650, Train_loss:0.19955
Step: 33700, Train_loss:0.215028
Step: 33750, Train_loss:0.207747
Step: 33800, Train_loss:0.288966
Step: 33850, Train_loss:0.263938
Step: 33900, Train_loss:0.274347
Step: 33950, Train_loss:0.225145
Step: 34000, Train_loss:0.250692
Step: 34050, Train_loss:0.263101
Step: 34100, Train_loss:0.23707
Step: 34150, Train_loss:0.27236
Step: 34200, Train_loss:0.185973
Step: 34250, Train_loss:0.205233
Step: 34300, Train_loss:0.245718
Step: 34350, Train_loss:0.202278
Step: 34400, Train_loss:0.180427
Step: 34450, Train_loss:0.264835
Step: 34500, Train_loss:0.225971
Step: 34550, Train_loss:0.244414
Step: 34600, Train_loss:0.218897
Step: 34650, Train_loss:0.208276
Step: 34700, Train_loss:0.196108
Step: 34750, Train_loss:0.200092
Step: 34800, Train_loss:0.264969
Step: 34850, Train_loss:0.223847
Step: 34900, Train_loss:0.224957
Step: 34950, Train_loss:0.232774
Step: 35000, Train_loss:0.249315
2018-10-25 00:03:17.163100 ---> Validation_loss: 1.19675
* test result update.
Step: 35050, Train_loss:0.199057
Step: 35100, Train_loss:0.206572
Step: 35150, Train_loss:0.191805
Step: 35200, Train_loss:0.20608
Step: 35250, Train_loss:0.239938
Step: 35300, Train_loss:0.226084
Step: 35350, Train_loss:0.268743
Step: 35400, Train_loss:0.23745
Step: 35450, Train_loss:0.172918
Step: 35500, Train_loss:0.253781
Step: 35550, Train_loss:0.182578
Step: 35600, Train_loss:0.177457
Step: 35650, Train_loss:0.213847
Step: 35700, Train_loss:0.260949
Step: 35750, Train_loss:0.239634
Step: 35800, Train_loss:0.282614
Step: 35850, Train_loss:0.25453
Step: 35900, Train_loss:0.239347
Step: 35950, Train_loss:0.232475
Step: 36000, Train_loss:0.19555
Step: 36050, Train_loss:0.204132
Step: 36100, Train_loss:0.24175
Step: 36150, Train_loss:0.248932
Step: 36200, Train_loss:0.209103
Step: 36250, Train_loss:0.233897
Step: 36300, Train_loss:0.27055
Step: 36350, Train_loss:0.256787
Step: 36400, Train_loss:0.162448
Step: 36450, Train_loss:0.20418
Step: 36500, Train_loss:0.180824
Step: 36550, Train_loss:0.187794
Step: 36600, Train_loss:0.180515
Step: 36650, Train_loss:0.24719
Step: 36700, Train_loss:0.270784
Step: 36750, Train_loss:0.225494
Step: 36800, Train_loss:0.244013
Step: 36850, Train_loss:0.288455
Step: 36900, Train_loss:0.168938
Step: 36950, Train_loss:0.184916
Step: 37000, Train_loss:0.287413
Step: 37050, Train_loss:0.192753
Step: 37100, Train_loss:0.225495
Step: 37150, Train_loss:0.233182
Step: 37200, Train_loss:0.319554
Step: 37250, Train_loss:0.202838
Step: 37300, Train_loss:0.198578
Step: 37350, Train_loss:0.198738
Step: 37400, Train_loss:0.258944
Step: 37450, Train_loss:0.202059
Step: 37500, Train_loss:0.204338
Step: 37550, Train_loss:0.206306
Step: 37600, Train_loss:0.207581
Step: 37650, Train_loss:0.177455
Step: 37700, Train_loss:0.212313
Step: 37750, Train_loss:0.21688
Step: 37800, Train_loss:0.211002
Step: 37850, Train_loss:0.214237
Step: 37900, Train_loss:0.228095
Step: 37950, Train_loss:0.170974
Step: 38000, Train_loss:0.250476
Step: 38050, Train_loss:0.233776
Step: 38100, Train_loss:0.190445
Step: 38150, Train_loss:0.214737
Step: 38200, Train_loss:0.205726
Step: 38250, Train_loss:0.248556
Step: 38300, Train_loss:0.321519
Step: 38350, Train_loss:0.218438
Step: 38400, Train_loss:0.275564
Step: 38450, Train_loss:0.203
Step: 38500, Train_loss:0.219235
Step: 38550, Train_loss:0.212261
Step: 38600, Train_loss:0.237657
Step: 38650, Train_loss:0.190168
Step: 38700, Train_loss:0.186195
Step: 38750, Train_loss:0.172782
Step: 38800, Train_loss:0.181337
Step: 38850, Train_loss:0.250111
Step: 38900, Train_loss:0.224652
Step: 38950, Train_loss:0.226495
Step: 39000, Train_loss:0.206754
Step: 39050, Train_loss:0.181411
Step: 39100, Train_loss:0.228414
Step: 39150, Train_loss:0.253934
****************** Epochs completed: 50******************
Step: 39200, Train_loss:0.231398
Step: 39250, Train_loss:0.245029
Step: 39300, Train_loss:0.176999
Step: 39350, Train_loss:0.203717
Step: 39400, Train_loss:0.243859
Step: 39450, Train_loss:0.206542
Step: 39500, Train_loss:0.224098
Step: 39550, Train_loss:0.211196
Step: 39600, Train_loss:0.232099
Step: 39650, Train_loss:0.224942
Step: 39700, Train_loss:0.250769
Step: 39750, Train_loss:0.189425
Step: 39800, Train_loss:0.24197
Step: 39850, Train_loss:0.181747
Step: 39900, Train_loss:0.226155
Step: 39950, Train_loss:0.171122
Step: 40000, Train_loss:0.212394
2018-10-25 00:30:44.081727 ---> Validation_loss: 1.1123
* test result update.
Step: 40050, Train_loss:0.197266
Step: 40100, Train_loss:0.207815
Step: 40150, Train_loss:0.178157
Step: 40200, Train_loss:0.261067
Step: 40250, Train_loss:0.196707
Step: 40300, Train_loss:0.227477
Step: 40350, Train_loss:0.22431
Step: 40400, Train_loss:0.184328
Step: 40450, Train_loss:0.269268
Step: 40500, Train_loss:0.183083
Step: 40550, Train_loss:0.212377
Step: 40600, Train_loss:0.157053
Step: 40650, Train_loss:0.244205
Step: 40700, Train_loss:0.190486
Step: 40750, Train_loss:0.213544
Step: 40800, Train_loss:0.19917
Step: 40850, Train_loss:0.222605
Step: 40900, Train_loss:0.175853
Step: 40950, Train_loss:0.197615
Step: 41000, Train_loss:0.194213
Step: 41050, Train_loss:0.2543
Step: 41100, Train_loss:0.212016
Step: 41150, Train_loss:0.261264
Step: 41200, Train_loss:0.269946
Step: 41250, Train_loss:0.261421
Step: 41300, Train_loss:0.236942
Step: 41350, Train_loss:0.240151
Step: 41400, Train_loss:0.241971
Step: 41450, Train_loss:0.211525
Step: 41500, Train_loss:0.259541
Step: 41550, Train_loss:0.178051
Step: 41600, Train_loss:0.20299
Step: 41650, Train_loss:0.228201
Step: 41700, Train_loss:0.196383
Step: 41750, Train_loss:0.206752
Step: 41800, Train_loss:0.238359
Step: 41850, Train_loss:0.274671
Step: 41900, Train_loss:0.236408
Step: 41950, Train_loss:0.215268
Step: 42000, Train_loss:0.187673
Step: 42050, Train_loss:0.183107
Step: 42100, Train_loss:0.204573
Step: 42150, Train_loss:0.222116
Step: 42200, Train_loss:0.252864
Step: 42250, Train_loss:0.253565
Step: 42300, Train_loss:0.199031
Step: 42350, Train_loss:0.172614
Step: 42400, Train_loss:0.228972
Step: 42450, Train_loss:0.240044
Step: 42500, Train_loss:0.217654
Step: 42550, Train_loss:0.178677
Step: 42600, Train_loss:0.213132
Step: 42650, Train_loss:0.200417
Step: 42700, Train_loss:0.244844
Step: 42750, Train_loss:0.251861
Step: 42800, Train_loss:0.235631
Step: 42850, Train_loss:0.226177
Step: 42900, Train_loss:0.227188
Step: 42950, Train_loss:0.237093
Step: 43000, Train_loss:0.228025
Step: 43050, Train_loss:0.210273
Step: 43100, Train_loss:0.243095
Step: 43150, Train_loss:0.229139
Step: 43200, Train_loss:0.216111
Step: 43250, Train_loss:0.234823
Step: 43300, Train_loss:0.252977
Step: 43350, Train_loss:0.174951
Step: 43400, Train_loss:0.185561
Step: 43450, Train_loss:0.201388
Step: 43500, Train_loss:0.197211
Step: 43550, Train_loss:0.209632
Step: 43600, Train_loss:0.218997
Step: 43650, Train_loss:0.216689
Step: 43700, Train_loss:0.177675
Step: 43750, Train_loss:0.229912
Step: 43800, Train_loss:0.216409
Step: 43850, Train_loss:0.207099
Step: 43900, Train_loss:0.203232
Step: 43950, Train_loss:0.27332
Step: 44000, Train_loss:0.22117
Step: 44050, Train_loss:0.231955
Step: 44100, Train_loss:0.190637
Step: 44150, Train_loss:0.170014
Step: 44200, Train_loss:0.185478
Step: 44250, Train_loss:0.21486
Step: 44300, Train_loss:0.190705
Step: 44350, Train_loss:0.21571
Step: 44400, Train_loss:0.199501
Step: 44450, Train_loss:0.246406
Step: 44500, Train_loss:0.239143
Step: 44550, Train_loss:0.259363
Step: 44600, Train_loss:0.172531
Step: 44650, Train_loss:0.246827
Step: 44700, Train_loss:0.238855
Step: 44750, Train_loss:0.250594
Step: 44800, Train_loss:0.2098
Step: 44850, Train_loss:0.162881
Step: 44900, Train_loss:0.185622
Step: 44950, Train_loss:0.205957
Step: 45000, Train_loss:0.198255
2018-10-25 00:58:09.268318 ---> Validation_loss: 1.1159
* test result update.
Step: 45050, Train_loss:0.191018
Step: 45100, Train_loss:0.230687
Step: 45150, Train_loss:0.190247
Step: 45200, Train_loss:0.208886
Step: 45250, Train_loss:0.207216
Step: 45300, Train_loss:0.238642
Step: 45350, Train_loss:0.162205
Step: 45400, Train_loss:0.172388
Step: 45450, Train_loss:0.237051
Step: 45500, Train_loss:0.184948
Step: 45550, Train_loss:0.167565
Step: 45600, Train_loss:0.194454
Step: 45650, Train_loss:0.278369
Step: 45700, Train_loss:0.16365
Step: 45750, Train_loss:0.244877
Step: 45800, Train_loss:0.200826
Step: 45850, Train_loss:0.195389
Step: 45900, Train_loss:0.198932
Step: 45950, Train_loss:0.179858
Step: 46000, Train_loss:0.20515
Step: 46050, Train_loss:0.195839
Step: 46100, Train_loss:0.187707
Step: 46150, Train_loss:0.204693
Step: 46200, Train_loss:0.203831
Step: 46250, Train_loss:0.229824
Step: 46300, Train_loss:0.218513
Step: 46350, Train_loss:0.177913
Step: 46400, Train_loss:0.190717
Step: 46450, Train_loss:0.204806
Step: 46500, Train_loss:0.223959
Step: 46550, Train_loss:0.179124
Step: 46600, Train_loss:0.164805
Step: 46650, Train_loss:0.229222
Step: 46700, Train_loss:0.202199
Step: 46750, Train_loss:0.198652
Step: 46800, Train_loss:0.174256
Step: 46850, Train_loss:0.257964
Step: 46900, Train_loss:0.178174
Step: 46950, Train_loss:0.187199
Step: 47000, Train_loss:0.265909
****************** Epochs completed: 60******************
Step: 47050, Train_loss:0.22103
Step: 47100, Train_loss:0.178585
Step: 47150, Train_loss:0.213029
Step: 47200, Train_loss:0.177687
Step: 47250, Train_loss:0.234922
Step: 47300, Train_loss:0.172867
Step: 47350, Train_loss:0.162016
Step: 47400, Train_loss:0.208976
Step: 47450, Train_loss:0.213265
Step: 47500, Train_loss:0.233434
Step: 47550, Train_loss:0.16926
Step: 47600, Train_loss:0.192138
Step: 47650, Train_loss:0.200195
Step: 47700, Train_loss:0.273445
Step: 47750, Train_loss:0.204284
Step: 47800, Train_loss:0.175701
Step: 47850, Train_loss:0.17952
Step: 47900, Train_loss:0.199791
Step: 47950, Train_loss:0.200028
Step: 48000, Train_loss:0.241951
Step: 48050, Train_loss:0.220888
Step: 48100, Train_loss:0.216997
Step: 48150, Train_loss:0.203568
Step: 48200, Train_loss:0.216402
Step: 48250, Train_loss:0.172676
Step: 48300, Train_loss:0.218633
Step: 48350, Train_loss:0.207549
Step: 48400, Train_loss:0.23574
Step: 48450, Train_loss:0.236687
Step: 48500, Train_loss:0.218065
Step: 48550, Train_loss:0.17698
Step: 48600, Train_loss:0.157929
Step: 48650, Train_loss:0.166375
Step: 48700, Train_loss:0.209262
Step: 48750, Train_loss:0.222759
Step: 48800, Train_loss:0.179202
Step: 48850, Train_loss:0.217666
Step: 48900, Train_loss:0.181917
Step: 48950, Train_loss:0.164728
Step: 49000, Train_loss:0.208256
Step: 49050, Train_loss:0.174741
Step: 49100, Train_loss:0.188037
Step: 49150, Train_loss:0.253793
Step: 49200, Train_loss:0.290202
Step: 49250, Train_loss:0.182981
Step: 49300, Train_loss:0.22816
Step: 49350, Train_loss:0.227375
Step: 49400, Train_loss:0.195261
Step: 49450, Train_loss:0.184728
Step: 49500, Train_loss:0.175963
Step: 49550, Train_loss:0.215194
Step: 49600, Train_loss:0.215054
Step: 49650, Train_loss:0.158552
Step: 49700, Train_loss:0.194228
Step: 49750, Train_loss:0.154892
Step: 49800, Train_loss:0.173087
Step: 49850, Train_loss:0.254435
Step: 49900, Train_loss:0.183056
Step: 49950, Train_loss:0.166077
Step: 50000, Train_loss:0.220562
2018-10-25 01:25:37.431081 ---> Validation_loss: 1.63305
****************** Epochs completed: 10******************
* test result update.
Step: 50050, Train_loss:0.178832
Step: 50100, Train_loss:0.23385
Step: 50150, Train_loss:0.197851
Step: 50200, Train_loss:0.233311
Step: 50250, Train_loss:0.19878
Step: 50300, Train_loss:0.165841
Step: 50350, Train_loss:0.248344
Step: 50400, Train_loss:0.230266
Step: 50450, Train_loss:0.193888
Step: 50500, Train_loss:0.181798
Step: 50550, Train_loss:0.180549
Step: 50600, Train_loss:0.201642
Step: 50650, Train_loss:0.160061
Step: 50700, Train_loss:0.176477
Step: 50750, Train_loss:0.218948
Step: 50800, Train_loss:0.170304
Step: 50850, Train_loss:0.216897
Step: 50900, Train_loss:0.228927
Step: 50950, Train_loss:0.206918
Step: 51000, Train_loss:0.172577
Step: 51050, Train_loss:0.172675
Step: 51100, Train_loss:0.197884
Step: 51150, Train_loss:0.202632
Step: 51200, Train_loss:0.186621
Step: 51250, Train_loss:0.189735
Step: 51300, Train_loss:0.28317
Step: 51350, Train_loss:0.228692
Step: 51400, Train_loss:0.25378
Step: 51450, Train_loss:0.208323
Step: 51500, Train_loss:0.186287
Step: 51550, Train_loss:0.277595
Step: 51600, Train_loss:0.226735
Step: 51650, Train_loss:0.249813
Step: 51700, Train_loss:0.176627
Step: 51750, Train_loss:0.166259
Step: 51800, Train_loss:0.164518
Step: 51850, Train_loss:0.205859
Step: 51900, Train_loss:0.211929
Step: 51950, Train_loss:0.214975
Step: 52000, Train_loss:0.186278
Step: 52050, Train_loss:0.248825
Step: 52100, Train_loss:0.23937
Step: 52150, Train_loss:0.173794
Step: 52200, Train_loss:0.212535
Step: 52250, Train_loss:0.189074
Step: 52300, Train_loss:0.170389
Step: 52350, Train_loss:0.213137
Step: 52400, Train_loss:0.182072
Step: 52450, Train_loss:0.224458
Step: 52500, Train_loss:0.170281
Step: 52550, Train_loss:0.1654
Step: 52600, Train_loss:0.216181
Step: 52650, Train_loss:0.200627
Step: 52700, Train_loss:0.191265
Step: 52750, Train_loss:0.185768
Step: 52800, Train_loss:0.179553
Step: 52850, Train_loss:0.221454
Step: 52900, Train_loss:0.188547
Step: 52950, Train_loss:0.212384
Step: 53000, Train_loss:0.158852
Step: 53050, Train_loss:0.180395
Step: 53100, Train_loss:0.199147
Step: 53150, Train_loss:0.167592
Step: 53200, Train_loss:0.18441
Step: 53250, Train_loss:0.242237
Step: 53300, Train_loss:0.233
Step: 53350, Train_loss:0.163669
Step: 53400, Train_loss:0.213349
Step: 53450, Train_loss:0.208057
Step: 53500, Train_loss:0.288703
Step: 53550, Train_loss:0.190111
Step: 53600, Train_loss:0.204601
Step: 53650, Train_loss:0.198063
Step: 53700, Train_loss:0.223371
Step: 53750, Train_loss:0.17433
Step: 53800, Train_loss:0.229698
Step: 53850, Train_loss:0.178632
Step: 53900, Train_loss:0.226203
Step: 53950, Train_loss:0.212869
Step: 54000, Train_loss:0.207359
Step: 54050, Train_loss:0.171692
Step: 54100, Train_loss:0.167484
Step: 54150, Train_loss:0.224694
Step: 54200, Train_loss:0.258514
Step: 54250, Train_loss:0.170373
Step: 54300, Train_loss:0.177251
Step: 54350, Train_loss:0.189028
Step: 54400, Train_loss:0.176806
Step: 54450, Train_loss:0.194804
Step: 54500, Train_loss:0.168024
Step: 54550, Train_loss:0.156355
Step: 54600, Train_loss:0.215807
Step: 54650, Train_loss:0.163489
Step: 54700, Train_loss:0.201054
Step: 54750, Train_loss:0.170597
Step: 54800, Train_loss:0.192688
Step: 54850, Train_loss:0.22737
****************** Epochs completed: 70******************
Step: 54900, Train_loss:0.193733
Step: 54950, Train_loss:0.213074
Step: 55000, Train_loss:0.170347
2018-10-25 01:53:06.947244 ---> Validation_loss: 1.43982
* test result update.
Step: 55050, Train_loss:0.20594
Step: 55100, Train_loss:0.246219
Step: 55150, Train_loss:0.255802
Step: 55200, Train_loss:0.286469
Step: 55250, Train_loss:0.232264
Step: 55300, Train_loss:0.18743
Step: 55350, Train_loss:0.178173
Step: 55400, Train_loss:0.229217
Step: 55450, Train_loss:0.180176
Step: 55500, Train_loss:0.226598
Step: 55550, Train_loss:0.232588
Step: 55600, Train_loss:0.252509
Step: 55650, Train_loss:0.26902
Step: 55700, Train_loss:0.218187
Step: 55750, Train_loss:0.166861
Step: 55800, Train_loss:0.220755
Step: 55850, Train_loss:0.22348
Step: 55900, Train_loss:0.233602
Step: 55950, Train_loss:0.205222
Step: 56000, Train_loss:0.15727
Step: 56050, Train_loss:0.216859
Step: 56100, Train_loss:0.218069
Step: 56150, Train_loss:0.163753
Step: 56200, Train_loss:0.169533
Step: 56250, Train_loss:0.24704
Step: 56300, Train_loss:0.171879
Step: 56350, Train_loss:0.223524
Step: 56400, Train_loss:0.226937
Step: 56450, Train_loss:0.202961
Step: 56500, Train_loss:0.194463
Step: 56550, Train_loss:0.207756
Step: 56600, Train_loss:0.169777
Step: 56650, Train_loss:0.212487
Step: 56700, Train_loss:0.140977
Step: 56750, Train_loss:0.159564
Step: 56800, Train_loss:0.246286
Step: 56850, Train_loss:0.180056
Step: 56900, Train_loss:0.222704
Step: 56950, Train_loss:0.219076
Step: 57000, Train_loss:0.231212
Step: 57050, Train_loss:0.19281
Step: 57100, Train_loss:0.23021
Step: 57150, Train_loss:0.182815
Step: 57200, Train_loss:0.18893
Step: 57250, Train_loss:0.198452
Step: 57300, Train_loss:0.195074
Step: 57350, Train_loss:0.191081
Step: 57400, Train_loss:0.163749
Step: 57450, Train_loss:0.170894
Step: 57500, Train_loss:0.163249
Step: 57550, Train_loss:0.191797
Step: 57600, Train_loss:0.160588
Step: 57650, Train_loss:0.187125
Step: 57700, Train_loss:0.176081
Step: 57750, Train_loss:0.182904
Step: 57800, Train_loss:0.272615
Step: 57850, Train_loss:0.172077
Step: 57900, Train_loss:0.176171
Step: 57950, Train_loss:0.171836
Step: 58000, Train_loss:0.21805
Step: 58050, Train_loss:0.215431
Step: 58100, Train_loss:0.208785
Step: 58150, Train_loss:0.160051
Step: 58200, Train_loss:0.175848
Step: 58250, Train_loss:0.234793
Step: 58300, Train_loss:0.209509
Step: 58350, Train_loss:0.175203
Step: 58400, Train_loss:0.22324
Step: 58450, Train_loss:0.222794
Step: 58500, Train_loss:0.207336
Step: 58550, Train_loss:0.194204
Step: 58600, Train_loss:0.177543
Step: 58650, Train_loss:0.283098
Step: 58700, Train_loss:0.194776
Step: 58750, Train_loss:0.191453
Step: 58800, Train_loss:0.208523
Step: 58850, Train_loss:0.2211
Step: 58900, Train_loss:0.173829
Step: 58950, Train_loss:0.222545
Step: 59000, Train_loss:0.181029
Step: 59050, Train_loss:0.188141
Step: 59100, Train_loss:0.284797
Step: 59150, Train_loss:0.217885
Step: 59200, Train_loss:0.155647
Step: 59250, Train_loss:0.16086
Step: 59300, Train_loss:0.212285
Step: 59350, Train_loss:0.196165
Step: 59400, Train_loss:0.22499
Step: 59450, Train_loss:0.15874
Step: 59500, Train_loss:0.173976
Step: 59550, Train_loss:0.1773
Step: 59600, Train_loss:0.215695
Step: 59650, Train_loss:0.181719
Step: 59700, Train_loss:0.14899
Step: 59750, Train_loss:0.222869
Step: 59800, Train_loss:0.16074
Step: 59850, Train_loss:0.191199
Step: 59900, Train_loss:0.233201
Step: 59950, Train_loss:0.173388
Step: 60000, Train_loss:0.196701
2018-10-25 02:20:35.887826 ---> Validation_loss: 1.89095
* test result update.
Step: 60050, Train_loss:0.242769
Step: 60100, Train_loss:0.153594
Step: 60150, Train_loss:0.217548
Step: 60200, Train_loss:0.279038
Step: 60250, Train_loss:0.203504
Step: 60300, Train_loss:0.177601
Step: 60350, Train_loss:0.190571
Step: 60400, Train_loss:0.235838
Step: 60450, Train_loss:0.156741
Step: 60500, Train_loss:0.158602
Step: 60550, Train_loss:0.188474
Step: 60600, Train_loss:0.20348
Step: 60650, Train_loss:0.232935
Step: 60700, Train_loss:0.145428
Step: 60750, Train_loss:0.237746
Step: 60800, Train_loss:0.160377
Step: 60850, Train_loss:0.235365
Step: 60900, Train_loss:0.168871
Step: 60950, Train_loss:0.242036
Step: 61000, Train_loss:0.206588
Step: 61050, Train_loss:0.263162
Step: 61100, Train_loss:0.16996
Step: 61150, Train_loss:0.200351
Step: 61200, Train_loss:0.156316
Step: 61250, Train_loss:0.238813
Step: 61300, Train_loss:0.176092
Step: 61350, Train_loss:0.171934
Step: 61400, Train_loss:0.223735
Step: 61450, Train_loss:0.185249
Step: 61500, Train_loss:0.181137
Step: 61550, Train_loss:0.225799
Step: 61600, Train_loss:0.24663
Step: 61650, Train_loss:0.153687
Step: 61700, Train_loss:0.165336
Step: 61750, Train_loss:0.167471
Step: 61800, Train_loss:0.222124
Step: 61850, Train_loss:0.185815
Step: 61900, Train_loss:0.212334
Step: 61950, Train_loss:0.245929
Step: 62000, Train_loss:0.199704
Step: 62050, Train_loss:0.181919
Step: 62100, Train_loss:0.229286
Step: 62150, Train_loss:0.183495
Step: 62200, Train_loss:0.24173
Step: 62250, Train_loss:0.226257
Step: 62300, Train_loss:0.164857
Step: 62350, Train_loss:0.145576
Step: 62400, Train_loss:0.172739
Step: 62450, Train_loss:0.19069
Step: 62500, Train_loss:0.271938
Step: 62550, Train_loss:0.254061
Step: 62600, Train_loss:0.233262
Step: 62650, Train_loss:0.180864
Step: 62700, Train_loss:0.207529
****************** Epochs completed: 80******************
Step: 62750, Train_loss:0.199073
Step: 62800, Train_loss:0.136136
Step: 62850, Train_loss:0.185264
Step: 62900, Train_loss:0.195798
Step: 62950, Train_loss:0.20132
Step: 63000, Train_loss:0.163923
Step: 63050, Train_loss:0.183916
Step: 63100, Train_loss:0.216571
Step: 63150, Train_loss:0.177198
Step: 63200, Train_loss:0.189299
Step: 63250, Train_loss:0.213823
Step: 63300, Train_loss:0.231923
Step: 63350, Train_loss:0.165435
Step: 63400, Train_loss:0.164051
Step: 63450, Train_loss:0.197948
Step: 63500, Train_loss:0.186832
Step: 63550, Train_loss:0.22283
Step: 63600, Train_loss:0.179553
Step: 63650, Train_loss:0.249153
Step: 63700, Train_loss:0.196963
Step: 63750, Train_loss:0.15362
Step: 63800, Train_loss:0.200414
Step: 63850, Train_loss:0.207225
Step: 63900, Train_loss:0.18032
Step: 63950, Train_loss:0.220433
Step: 64000, Train_loss:0.161514
Step: 64050, Train_loss:0.166216
Step: 64100, Train_loss:0.176129
Step: 64150, Train_loss:0.18887
Step: 64200, Train_loss:0.157506
Step: 64250, Train_loss:0.190581
Step: 64300, Train_loss:0.177684
Step: 64350, Train_loss:0.174177
Step: 64400, Train_loss:0.254152
Step: 64450, Train_loss:0.18287
Step: 64500, Train_loss:0.192955
Step: 64550, Train_loss:0.15556
Step: 64600, Train_loss:0.211068
Step: 64650, Train_loss:0.225227
Step: 64700, Train_loss:0.164138
Step: 64750, Train_loss:0.197198
Step: 64800, Train_loss:0.184695
Step: 64850, Train_loss:0.19504
Step: 64900, Train_loss:0.155721
Step: 64950, Train_loss:0.20746
Step: 65000, Train_loss:0.163525
2018-10-25 02:48:16.350924 ---> Validation_loss: 1.75847
* test result update.
Step: 65050, Train_loss:0.191678
Step: 65100, Train_loss:0.163239
Step: 65150, Train_loss:0.193641
Step: 65200, Train_loss:0.152014
Step: 65250, Train_loss:0.155114
Step: 65300, Train_loss:0.205806
Step: 65350, Train_loss:0.151086
Step: 65400, Train_loss:0.18365
Step: 65450, Train_loss:0.175656
Step: 65500, Train_loss:0.156082
Step: 65550, Train_loss:0.181888
Step: 65600, Train_loss:0.172471
Step: 65650, Train_loss:0.191207
Step: 65700, Train_loss:0.232113
Step: 65750, Train_loss:0.161133
Step: 65800, Train_loss:0.173097
Step: 65850, Train_loss:0.211619
Step: 65900, Train_loss:0.210036
Step: 65950, Train_loss:0.210241
Step: 66000, Train_loss:0.152189
Step: 66050, Train_loss:0.174815
Step: 66100, Train_loss:0.180017
Step: 66150, Train_loss:0.178129
Step: 66200, Train_loss:0.189372
Step: 66250, Train_loss:0.20098
Step: 66300, Train_loss:0.206344
Step: 66350, Train_loss:0.185684
Step: 66400, Train_loss:0.204744
Step: 66450, Train_loss:0.225699
Step: 66500, Train_loss:0.15811
Step: 66550, Train_loss:0.234851
Step: 66600, Train_loss:0.194523
Step: 66650, Train_loss:0.182925
Step: 66700, Train_loss:0.189903
Step: 66750, Train_loss:0.211724
Step: 66800, Train_loss:0.13866
Step: 66850, Train_loss:0.159982
Step: 66900, Train_loss:0.151297
Step: 66950, Train_loss:0.185878
Step: 67000, Train_loss:0.162723
Step: 67050, Train_loss:0.185913
Step: 67100, Train_loss:0.184953
Step: 67150, Train_loss:0.183399
Step: 67200, Train_loss:0.207361
Step: 67250, Train_loss:0.220522
Step: 67300, Train_loss:0.174153
Step: 67350, Train_loss:0.229689
Step: 67400, Train_loss:0.177558
Step: 67450, Train_loss:0.163712
Step: 67500, Train_loss:0.254308
Step: 67550, Train_loss:0.163977
Step: 67600, Train_loss:0.190384
Step: 67650, Train_loss:0.189872
Step: 67700, Train_loss:0.199156
Step: 67750, Train_loss:0.141392
Step: 67800, Train_loss:0.193583
Step: 67850, Train_loss:0.200828
Step: 67900, Train_loss:0.192757
Step: 67950, Train_loss:0.168046
Step: 68000, Train_loss:0.21086
Step: 68050, Train_loss:0.195376
Step: 68100, Train_loss:0.211748
Step: 68150, Train_loss:0.148701
Step: 68200, Train_loss:0.137404
Step: 68250, Train_loss:0.195494
Step: 68300, Train_loss:0.142559
Step: 68350, Train_loss:0.212802
Step: 68400, Train_loss:0.183222
Step: 68450, Train_loss:0.175489
Step: 68500, Train_loss:0.212727
Step: 68550, Train_loss:0.169597
Step: 68600, Train_loss:0.159909
Step: 68650, Train_loss:0.17933
Step: 68700, Train_loss:0.196236
Step: 68750, Train_loss:0.140083
Step: 68800, Train_loss:0.206228
Step: 68850, Train_loss:0.168906
Step: 68900, Train_loss:0.211281
Step: 68950, Train_loss:0.251606
Step: 69000, Train_loss:0.183881
Step: 69050, Train_loss:0.188884
Step: 69100, Train_loss:0.192425
Step: 69150, Train_loss:0.17454
Step: 69200, Train_loss:0.180821
Step: 69250, Train_loss:0.205386
Step: 69300, Train_loss:0.206078
Step: 69350, Train_loss:0.154986
Step: 69400, Train_loss:0.160031
Step: 69450, Train_loss:0.198506
Step: 69500, Train_loss:0.230584
Step: 69550, Train_loss:0.149087
Step: 69600, Train_loss:0.268925
Step: 69650, Train_loss:0.187519
Step: 69700, Train_loss:0.213026
Step: 69750, Train_loss:0.227957
Step: 69800, Train_loss:0.24939
Step: 69850, Train_loss:0.189603
Step: 69900, Train_loss:0.139874
Step: 69950, Train_loss:0.177664
Step: 70000, Train_loss:0.219624
2018-10-25 03:15:47.643226 ---> Validation_loss: 2.59845
* test result update.
Step: 70050, Train_loss:0.207683
Step: 70100, Train_loss:0.159062
Step: 70150, Train_loss:0.225845
Step: 70200, Train_loss:0.157166
Step: 70250, Train_loss:0.202725
Step: 70300, Train_loss:0.197184
Step: 70350, Train_loss:0.202521
Step: 70400, Train_loss:0.202121
Step: 70450, Train_loss:0.184054
Step: 70500, Train_loss:0.20904
Step: 70550, Train_loss:0.159891
****************** Epochs completed: 90******************
Step: 70600, Train_loss:0.166698
Step: 70650, Train_loss:0.174039
Step: 70700, Train_loss:0.206591
Step: 70750, Train_loss:0.189462
Step: 70800, Train_loss:0.202643
Step: 70850, Train_loss:0.208331
Step: 70900, Train_loss:0.205964
Step: 70950, Train_loss:0.180475
Step: 71000, Train_loss:0.260745
Step: 71050, Train_loss:0.154053
Step: 71100, Train_loss:0.146968
Step: 71150, Train_loss:0.193282
Step: 71200, Train_loss:0.190924
Step: 71250, Train_loss:0.22196
Step: 71300, Train_loss:0.18737
Step: 71350, Train_loss:0.165905
Step: 71400, Train_loss:0.162737
Step: 71450, Train_loss:0.191172
Step: 71500, Train_loss:0.15391
Step: 71550, Train_loss:0.230523
Step: 71600, Train_loss:0.211255
Step: 71650, Train_loss:0.133443
Step: 71700, Train_loss:0.169466
Step: 71750, Train_loss:0.197739
Step: 71800, Train_loss:0.142464
Step: 71850, Train_loss:0.194938
Step: 71900, Train_loss:0.150749
Step: 71950, Train_loss:0.157331
Step: 72000, Train_loss:0.17581
Step: 72050, Train_loss:0.145521
Step: 72100, Train_loss:0.181447
Step: 72150, Train_loss:0.184187
Step: 72200, Train_loss:0.137986
Step: 72250, Train_loss:0.16593
Step: 72300, Train_loss:0.207575
Step: 72350, Train_loss:0.192188
Step: 72400, Train_loss:0.188828
Step: 72450, Train_loss:0.200218
Step: 72500, Train_loss:0.152593
Step: 72550, Train_loss:0.160946
Step: 72600, Train_loss:0.217125
Step: 72650, Train_loss:0.230396
Step: 72700, Train_loss:0.193619
Step: 72750, Train_loss:0.201415
Step: 72800, Train_loss:0.170217
Step: 72850, Train_loss:0.138614
Step: 72900, Train_loss:0.165272
Step: 72950, Train_loss:0.166841
Step: 73000, Train_loss:0.206543
Step: 73050, Train_loss:0.166298
Step: 73100, Train_loss:0.186489
Step: 73150, Train_loss:0.169604
Step: 73200, Train_loss:0.21777
Step: 73250, Train_loss:0.184214
Step: 73300, Train_loss:0.209498
Step: 73350, Train_loss:0.243901
Step: 73400, Train_loss:0.248802
Step: 73450, Train_loss:0.179749
Step: 73500, Train_loss:0.200262
Step: 73550, Train_loss:0.182786
Step: 73600, Train_loss:0.188369
Step: 73650, Train_loss:0.147643
Step: 73700, Train_loss:0.152302
Step: 73750, Train_loss:0.16255
Step: 73800, Train_loss:0.192239
Step: 73850, Train_loss:0.191303
Step: 73900, Train_loss:0.158263
Step: 73950, Train_loss:0.184888
Step: 74000, Train_loss:0.183054
Step: 74050, Train_loss:0.168919
Step: 74100, Train_loss:0.159854
Step: 74150, Train_loss:0.195159
Step: 74200, Train_loss:0.180975
Step: 74250, Train_loss:0.188689
Step: 74300, Train_loss:0.165538
Step: 74350, Train_loss:0.151733
Step: 74400, Train_loss:0.207915
Step: 74450, Train_loss:0.177768
Step: 74500, Train_loss:0.174757
Step: 74550, Train_loss:0.216417
Step: 74600, Train_loss:0.206189
Step: 74650, Train_loss:0.159446
Step: 74700, Train_loss:0.203827
Step: 74750, Train_loss:0.211817
Step: 74800, Train_loss:0.161871
Step: 74850, Train_loss:0.204464
Step: 74900, Train_loss:0.159798
Step: 74950, Train_loss:0.158751
Step: 75000, Train_loss:0.168687
2018-10-25 03:43:19.920620 ---> Validation_loss: 2.58351
* test result update.
Step: 75050, Train_loss:0.177924
Step: 75100, Train_loss:0.189111
Step: 75150, Train_loss:0.171325
Step: 75200, Train_loss:0.205478
Step: 75250, Train_loss:0.222736
Step: 75300, Train_loss:0.201188
Step: 75350, Train_loss:0.192753
Step: 75400, Train_loss:0.254366
Step: 75450, Train_loss:0.16271
Step: 75500, Train_loss:0.173543
Step: 75550, Train_loss:0.171023
Step: 75600, Train_loss:0.186476
Step: 75650, Train_loss:0.239474
Step: 75700, Train_loss:0.177981
Step: 75750, Train_loss:0.18994
Step: 75800, Train_loss:0.156803
Step: 75850, Train_loss:0.151951
Step: 75900, Train_loss:0.151215
Step: 75950, Train_loss:0.139722
Step: 76000, Train_loss:0.203559
Step: 76050, Train_loss:0.234096
Step: 76100, Train_loss:0.176379
Step: 76150, Train_loss:0.193336
Step: 76200, Train_loss:0.163232
Step: 76250, Train_loss:0.170791
Step: 76300, Train_loss:0.162062
Step: 76350, Train_loss:0.154157
Step: 76400, Train_loss:0.178929
Step: 76450, Train_loss:0.171967
Step: 76500, Train_loss:0.136312
Step: 76550, Train_loss:0.130138
Step: 76600, Train_loss:0.214357
Step: 76650, Train_loss:0.180574
Step: 76700, Train_loss:0.183499
Step: 76750, Train_loss:0.21499
Step: 76800, Train_loss:0.161307
Step: 76850, Train_loss:0.161742
Step: 76900, Train_loss:0.18377
Step: 76950, Train_loss:0.169379
Step: 77000, Train_loss:0.151418
Step: 77050, Train_loss:0.170048
Step: 77100, Train_loss:0.164394
Step: 77150, Train_loss:0.186141
Step: 77200, Train_loss:0.142698
Step: 77250, Train_loss:0.186264
Step: 77300, Train_loss:0.184833
Step: 77350, Train_loss:0.170192
Step: 77400, Train_loss:0.159903
Step: 77450, Train_loss:0.179629
Step: 77500, Train_loss:0.161144
Step: 77550, Train_loss:0.144873
Step: 77600, Train_loss:0.174818
Step: 77650, Train_loss:0.153117
Step: 77700, Train_loss:0.206352
Step: 77750, Train_loss:0.198301
Step: 77800, Train_loss:0.131849
Step: 77850, Train_loss:0.205432
Step: 77900, Train_loss:0.156397
Step: 77950, Train_loss:0.214163
Step: 78000, Train_loss:0.231667
Step: 78050, Train_loss:0.181886
Step: 78100, Train_loss:0.186248
Step: 78150, Train_loss:0.158542
Step: 78200, Train_loss:0.146052
Step: 78250, Train_loss:0.181833
Step: 78300, Train_loss:0.18211
Step: 78350, Train_loss:0.151482
****************** Epochs completed: 100******************
Step: 78400, Train_loss:0.168175
Step: 78450, Train_loss:0.178507
Step: 78500, Train_loss:0.160114
Step: 78550, Train_loss:0.208247
Step: 78600, Train_loss:0.140324
Step: 78650, Train_loss:0.198894
Step: 78700, Train_loss:0.191903
Step: 78750, Train_loss:0.185962
Step: 78800, Train_loss:0.212996
Step: 78850, Train_loss:0.174202
Step: 78900, Train_loss:0.218187
Step: 78950, Train_loss:0.148475
Step: 79000, Train_loss:0.215786
Step: 79050, Train_loss:0.213408
Step: 79100, Train_loss:0.21236
Step: 79150, Train_loss:0.158695
Step: 79200, Train_loss:0.176822
Step: 79250, Train_loss:0.190712
Step: 79300, Train_loss:0.195519
Step: 79350, Train_loss:0.179995
Step: 79400, Train_loss:0.229797
Step: 79450, Train_loss:0.178481
Step: 79500, Train_loss:0.20912
Step: 79550, Train_loss:0.137156
Step: 79600, Train_loss:0.175095
Step: 79650, Train_loss:0.160787
Step: 79700, Train_loss:0.152762
Step: 79750, Train_loss:0.151749
Step: 79800, Train_loss:0.208958
Step: 79850, Train_loss:0.213565
Step: 79900, Train_loss:0.159348
Step: 79950, Train_loss:0.200653
Step: 80000, Train_loss:0.146903
2018-10-25 04:10:52.064459 ---> Validation_loss: 3.15523
* test result update.
Step: 80050, Train_loss:0.144003
Step: 80100, Train_loss:0.163548
Step: 80150, Train_loss:0.155289
Step: 80200, Train_loss:0.19474
Step: 80250, Train_loss:0.166427
Step: 80300, Train_loss:0.194726
Step: 80350, Train_loss:0.171188
Step: 80400, Train_loss:0.170111
Step: 80450, Train_loss:0.16223
Step: 80500, Train_loss:0.175122
Step: 80550, Train_loss:0.165547
Step: 80600, Train_loss:0.154643
Step: 80650, Train_loss:0.188604
Step: 80700, Train_loss:0.204359
Step: 80750, Train_loss:0.218398
Step: 80800, Train_loss:0.15834
Step: 80850, Train_loss:0.179847
Step: 80900, Train_loss:0.166052
Step: 80950, Train_loss:0.171861
Step: 81000, Train_loss:0.196022
Step: 81050, Train_loss:0.171128
Step: 81100, Train_loss:0.174789
Step: 81150, Train_loss:0.145343
Step: 81200, Train_loss:0.15712
Step: 81250, Train_loss:0.179755
Step: 81300, Train_loss:0.161771
Step: 81350, Train_loss:0.209682
Step: 81400, Train_loss:0.212207
Step: 81450, Train_loss:0.198949
Step: 81500, Train_loss:0.234179
Step: 81550, Train_loss:0.152979
Step: 81600, Train_loss:0.185409
Step: 81650, Train_loss:0.218169
Step: 81700, Train_loss:0.187158
Step: 81750, Train_loss:0.174307
Step: 81800, Train_loss:0.188373
Step: 81850, Train_loss:0.185865
Step: 81900, Train_loss:0.188681
Step: 81950, Train_loss:0.151272
Step: 82000, Train_loss:0.218165
Step: 82050, Train_loss:0.137531
Step: 82100, Train_loss:0.197148
Step: 82150, Train_loss:0.194691
Step: 82200, Train_loss:0.216171
Step: 82250, Train_loss:0.166429
Step: 82300, Train_loss:0.216583
Step: 82350, Train_loss:0.160574
Step: 82400, Train_loss:0.196206
Step: 82450, Train_loss:0.229757
Step: 82500, Train_loss:0.139008
Step: 82550, Train_loss:0.214286
Step: 82600, Train_loss:0.159085
Step: 82650, Train_loss:0.195386
Step: 82700, Train_loss:0.180836
Step: 82750, Train_loss:0.202407
Step: 82800, Train_loss:0.208698
Step: 82850, Train_loss:0.189614
Step: 82900, Train_loss:0.221049
Step: 82950, Train_loss:0.204141
Step: 83000, Train_loss:0.187426
Step: 83050, Train_loss:0.143852
Step: 83100, Train_loss:0.21585
Step: 83150, Train_loss:0.145165
Step: 83200, Train_loss:0.155953
Step: 83250, Train_loss:0.178489
Step: 83300, Train_loss:0.163638
Step: 83350, Train_loss:0.214464
Step: 83400, Train_loss:0.161499
Step: 83450, Train_loss:0.147001
Step: 83500, Train_loss:0.239957
Step: 83550, Train_loss:0.136748
Step: 83600, Train_loss:0.182267
Step: 83650, Train_loss:0.179773
Step: 83700, Train_loss:0.171443
Step: 83750, Train_loss:0.174476
Step: 83800, Train_loss:0.215817
Step: 83850, Train_loss:0.175142
Step: 83900, Train_loss:0.188253
Step: 83950, Train_loss:0.179831
Step: 84000, Train_loss:0.211658
Step: 84050, Train_loss:0.150685
Step: 84100, Train_loss:0.176398
Step: 84150, Train_loss:0.156006
Step: 84200, Train_loss:0.165517
Step: 84250, Train_loss:0.154988
Step: 84300, Train_loss:0.16182
Step: 84350, Train_loss:0.189854
Step: 84400, Train_loss:0.236748
Step: 84450, Train_loss:0.120133
Step: 84500, Train_loss:0.143048
Step: 84550, Train_loss:0.199027
Step: 84600, Train_loss:0.144468
Step: 84650, Train_loss:0.163004
Step: 84700, Train_loss:0.182974
Step: 84750, Train_loss:0.18425
Step: 84800, Train_loss:0.183237
Step: 84850, Train_loss:0.175427
Step: 84900, Train_loss:0.17778
Step: 84950, Train_loss:0.167713
Step: 85000, Train_loss:0.145757
2018-10-25 04:38:23.056406 ---> Validation_loss: 2.76998
* test result update.
Step: 85050, Train_loss:0.197062
Step: 85100, Train_loss:0.143218
Step: 85150, Train_loss:0.143208
Step: 85200, Train_loss:0.167011
Step: 85250, Train_loss:0.169946
Step: 85300, Train_loss:0.175248
Step: 85350, Train_loss:0.203022
Step: 85400, Train_loss:0.180184
Step: 85450, Train_loss:0.16701
Step: 85500, Train_loss:0.145645
Step: 85550, Train_loss:0.161962
Step: 85600, Train_loss:0.164796
Step: 85650, Train_loss:0.191938
Step: 85700, Train_loss:0.174509
Step: 85750, Train_loss:0.187248
Step: 85800, Train_loss:0.163471
Step: 85850, Train_loss:0.18606
Step: 85900, Train_loss:0.138911
Step: 85950, Train_loss:0.207062
Step: 86000, Train_loss:0.137981
Step: 86050, Train_loss:0.157811
Step: 86100, Train_loss:0.153572
Step: 86150, Train_loss:0.13539
Step: 86200, Train_loss:0.167868
****************** Epochs completed: 110******************
Step: 86250, Train_loss:0.17607
Step: 86300, Train_loss:0.157041
Step: 86350, Train_loss:0.188797
Step: 86400, Train_loss:0.187348
Step: 86450, Train_loss:0.177646
Step: 86500, Train_loss:0.169201
Step: 86550, Train_loss:0.136261
Step: 86600, Train_loss:0.164446
Step: 86650, Train_loss:0.1569
Step: 86700, Train_loss:0.180295
Step: 86750, Train_loss:0.232829
Step: 86800, Train_loss:0.141666
Step: 86850, Train_loss:0.198308
Step: 86900, Train_loss:0.178843
Step: 86950, Train_loss:0.163084
Step: 87000, Train_loss:0.199916
Step: 87050, Train_loss:0.167254
Step: 87100, Train_loss:0.201254
Step: 87150, Train_loss:0.18985
Step: 87200, Train_loss:0.157957
Step: 87250, Train_loss:0.141374
Step: 87300, Train_loss:0.173701
Step: 87350, Train_loss:0.155247
Step: 87400, Train_loss:0.187226
Step: 87450, Train_loss:0.212387
Step: 87500, Train_loss:0.157784
Step: 87550, Train_loss:0.156969
Step: 87600, Train_loss:0.219522
Step: 87650, Train_loss:0.15308
Step: 87700, Train_loss:0.145605
Step: 87750, Train_loss:0.169056
Step: 87800, Train_loss:0.176981
Step: 87850, Train_loss:0.158215
Step: 87900, Train_loss:0.183213
Step: 87950, Train_loss:0.169893
Step: 88000, Train_loss:0.156012
Step: 88050, Train_loss:0.161815
Step: 88100, Train_loss:0.171933
Step: 88150, Train_loss:0.164065
Step: 88200, Train_loss:0.221885
Step: 88250, Train_loss:0.164274
Step: 88300, Train_loss:0.155436
Step: 88350, Train_loss:0.158812
Step: 88400, Train_loss:0.161563
Step: 88450, Train_loss:0.177616
Step: 88500, Train_loss:0.157027
Step: 88550, Train_loss:0.158813
Step: 88600, Train_loss:0.182231
Step: 88650, Train_loss:0.203438
Step: 88700, Train_loss:0.229208
Step: 88750, Train_loss:0.15694
Step: 88800, Train_loss:0.193433
Step: 88850, Train_loss:0.185451
Step: 88900, Train_loss:0.18284
Step: 88950, Train_loss:0.147843
Step: 89000, Train_loss:0.205731
Step: 89050, Train_loss:0.151431
Step: 89100, Train_loss:0.156282
Step: 89150, Train_loss:0.169829
Step: 89200, Train_loss:0.159501
Step: 89250, Train_loss:0.144369
Step: 89300, Train_loss:0.147223
Step: 89350, Train_loss:0.164631
Step: 89400, Train_loss:0.18083
Step: 89450, Train_loss:0.142516
Step: 89500, Train_loss:0.167303
Step: 89550, Train_loss:0.136052
Step: 89600, Train_loss:0.186478
Step: 89650, Train_loss:0.146879
Step: 89700, Train_loss:0.156348
Step: 89750, Train_loss:0.15674
Step: 89800, Train_loss:0.16366
Step: 89850, Train_loss:0.180786
Step: 89900, Train_loss:0.17591
Step: 89950, Train_loss:0.151318
Step: 90000, Train_loss:0.159815
2018-10-25 05:05:55.001850 ---> Validation_loss: 3.29007
* test result update.
Step: 90050, Train_loss:0.177264
Step: 90100, Train_loss:0.126591
Step: 90150, Train_loss:0.132525
Step: 90200, Train_loss:0.206722
Step: 90250, Train_loss:0.186878
Step: 90300, Train_loss:0.194672
Step: 90350, Train_loss:0.183302
Step: 90400, Train_loss:0.189889
Step: 90450, Train_loss:0.190192
Step: 90500, Train_loss:0.163637
Step: 90550, Train_loss:0.155058
Step: 90600, Train_loss:0.164049
Step: 90650, Train_loss:0.167735
Step: 90700, Train_loss:0.172746
Step: 90750, Train_loss:0.190006
Step: 90800, Train_loss:0.155701
Step: 90850, Train_loss:0.220063
Step: 90900, Train_loss:0.139519
Step: 90950, Train_loss:0.144019
Step: 91000, Train_loss:0.183491
Step: 91050, Train_loss:0.125119
Step: 91100, Train_loss:0.170759
Step: 91150, Train_loss:0.184727
Step: 91200, Train_loss:0.18944
Step: 91250, Train_loss:0.187488
Step: 91300, Train_loss:0.17181
Step: 91350, Train_loss:0.182238
Step: 91400, Train_loss:0.204582
Step: 91450, Train_loss:0.179923
Step: 91500, Train_loss:0.176426
Step: 91550, Train_loss:0.170341
Step: 91600, Train_loss:0.185419
Step: 91650, Train_loss:0.182184
Step: 91700, Train_loss:0.166652
Step: 91750, Train_loss:0.13701
Step: 91800, Train_loss:0.155189
Step: 91850, Train_loss:0.153702
Step: 91900, Train_loss:0.124219
Step: 91950, Train_loss:0.137595
Step: 92000, Train_loss:0.132694
Step: 92050, Train_loss:0.202416
Step: 92100, Train_loss:0.190103
Step: 92150, Train_loss:0.184723
Step: 92200, Train_loss:0.130629
Step: 92250, Train_loss:0.13596
Step: 92300, Train_loss:0.20137
Step: 92350, Train_loss:0.186668
Step: 92400, Train_loss:0.162748
Step: 92450, Train_loss:0.1958
Step: 92500, Train_loss:0.141707
Step: 92550, Train_loss:0.157733
Step: 92600, Train_loss:0.128657
Step: 92650, Train_loss:0.157292
Step: 92700, Train_loss:0.16294
Step: 92750, Train_loss:0.184972
Step: 92800, Train_loss:0.166654
Step: 92850, Train_loss:0.140826
Step: 92900, Train_loss:0.197793
Step: 92950, Train_loss:0.123522
Step: 93000, Train_loss:0.165735
Step: 93050, Train_loss:0.15361
Step: 93100, Train_loss:0.163257
Step: 93150, Train_loss:0.157381
Step: 93200, Train_loss:0.172087
Step: 93250, Train_loss:0.195723
Step: 93300, Train_loss:0.176976
Step: 93350, Train_loss:0.210696
Step: 93400, Train_loss:0.185815
Step: 93450, Train_loss:0.164817
Step: 93500, Train_loss:0.169884
Step: 93550, Train_loss:0.196608
Step: 93600, Train_loss:0.189819
Step: 93650, Train_loss:0.203442
Step: 93700, Train_loss:0.163079
Step: 93750, Train_loss:0.155985
Step: 93800, Train_loss:0.140703
Step: 93850, Train_loss:0.15929
Step: 93900, Train_loss:0.155982
Step: 93950, Train_loss:0.139707
Step: 94000, Train_loss:0.203616
Step: 94050, Train_loss:0.203101
****************** Epochs completed: 120******************
Step: 94100, Train_loss:0.188487
Step: 94150, Train_loss:0.185659
Step: 94200, Train_loss:0.139949
Step: 94250, Train_loss:0.178914
Step: 94300, Train_loss:0.163275
Step: 94350, Train_loss:0.156623
Step: 94400, Train_loss:0.175495
Step: 94450, Train_loss:0.149489
Step: 94500, Train_loss:0.199338
Step: 94550, Train_loss:0.137402
Step: 94600, Train_loss:0.134781
Step: 94650, Train_loss:0.155074
Step: 94700, Train_loss:0.157538
Step: 94750, Train_loss:0.153531
Step: 94800, Train_loss:0.169916
Step: 94850, Train_loss:0.135738
Step: 94900, Train_loss:0.130089
Step: 94950, Train_loss:0.176239
Step: 95000, Train_loss:0.154307
2018-10-25 05:33:25.379790 ---> Validation_loss: 3.52215
* test result update.
Step: 95050, Train_loss:0.13953
Step: 95100, Train_loss:0.170523
Step: 95150, Train_loss:0.188717
Step: 95200, Train_loss:0.161555
Step: 95250, Train_loss:0.186014
Step: 95300, Train_loss:0.151989
Step: 95350, Train_loss:0.154208
Step: 95400, Train_loss:0.149612
Step: 95450, Train_loss:0.179706
Step: 95500, Train_loss:0.132251
Step: 95550, Train_loss:0.156363
Step: 95600, Train_loss:0.173122
Step: 95650, Train_loss:0.153589
Step: 95700, Train_loss:0.149474
Step: 95750, Train_loss:0.168143
Step: 95800, Train_loss:0.161009
Step: 95850, Train_loss:0.145768
Step: 95900, Train_loss:0.169544
Step: 95950, Train_loss:0.211237
Step: 96000, Train_loss:0.224946
Step: 96050, Train_loss:0.211861
Step: 96100, Train_loss:0.186325
Step: 96150, Train_loss:0.133585
Step: 96200, Train_loss:0.166523
Step: 96250, Train_loss:0.143109
Step: 96300, Train_loss:0.179404
Step: 96350, Train_loss:0.166686
Step: 96400, Train_loss:0.2102
Step: 96450, Train_loss:0.202644
Step: 96500, Train_loss:0.150626
Step: 96550, Train_loss:0.156705
Step: 96600, Train_loss:0.145998
Step: 96650, Train_loss:0.197447
Step: 96700, Train_loss:0.162786
Step: 96750, Train_loss:0.15093
Step: 96800, Train_loss:0.116942
Step: 96850, Train_loss:0.251063
Step: 96900, Train_loss:0.160987
Step: 96950, Train_loss:0.151341
Step: 97000, Train_loss:0.199692
Step: 97050, Train_loss:0.165517
Step: 97100, Train_loss:0.142175
Step: 97150, Train_loss:0.164675
Step: 97200, Train_loss:0.176434
Step: 97250, Train_loss:0.143462
Step: 97300, Train_loss:0.177425
Step: 97350, Train_loss:0.143949
Step: 97400, Train_loss:0.14038
Step: 97450, Train_loss:0.209268
Step: 97500, Train_loss:0.176933
Step: 97550, Train_loss:0.191432
Step: 97600, Train_loss:0.174427
Step: 97650, Train_loss:0.166624
Step: 97700, Train_loss:0.145739
Step: 97750, Train_loss:0.198798
Step: 97800, Train_loss:0.131597
Step: 97850, Train_loss:0.222395
Step: 97900, Train_loss:0.152092
Step: 97950, Train_loss:0.205931
Step: 98000, Train_loss:0.163325
Step: 98050, Train_loss:0.180161
Step: 98100, Train_loss:0.242983
Step: 98150, Train_loss:0.196221
Step: 98200, Train_loss:0.19502
Step: 98250, Train_loss:0.136854
Step: 98300, Train_loss:0.181815
Step: 98350, Train_loss:0.147829
Step: 98400, Train_loss:0.170566
Step: 98450, Train_loss:0.147153
Step: 98500, Train_loss:0.162118
Step: 98550, Train_loss:0.147395
Step: 98600, Train_loss:0.177659
Step: 98650, Train_loss:0.141122
Step: 98700, Train_loss:0.17565
Step: 98750, Train_loss:0.180593
Step: 98800, Train_loss:0.160258
Step: 98850, Train_loss:0.117361
Step: 98900, Train_loss:0.145729
Step: 98950, Train_loss:0.174224
Step: 99000, Train_loss:0.181759
Step: 99050, Train_loss:0.155056
Step: 99100, Train_loss:0.133158
Step: 99150, Train_loss:0.162112
Step: 99200, Train_loss:0.173527
Step: 99250, Train_loss:0.165025
Step: 99300, Train_loss:0.149207
Step: 99350, Train_loss:0.183192
Step: 99400, Train_loss:0.145178
Step: 99450, Train_loss:0.149281
Step: 99500, Train_loss:0.120911
Step: 99550, Train_loss:0.139717
Step: 99600, Train_loss:0.176114
Step: 99650, Train_loss:0.141665
Step: 99700, Train_loss:0.170622
Step: 99750, Train_loss:0.184076
Step: 99800, Train_loss:0.129428
Step: 99850, Train_loss:0.148989
Step: 99900, Train_loss:0.135196
Step: 99950, Train_loss:0.162534
Step: 100000, Train_loss:0.199521
2018-10-25 06:00:56.416392 ---> Validation_loss: 3.91803
